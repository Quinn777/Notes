论文名称：Self-Challenging Improves Cross-Domain Generalization

论文地址：https://arxiv.org/pdf/2007.02454v1.pdf

代码地址：https://github.com/DeLightCMU/RSC



## 写在前面

本科时看的第一篇文章就是domain adaptation相关，当时就看的一脸懵逼，这次再接触到domain generation方向的文章感觉好理解多了（虽然还有大段的证明看不懂）。回过头来想想本文中的RSC实质上就是一种高级剪枝法，或者说是DropOut，不过作者只选取了最后一层特征进行修剪就达到了出乎意料的效果，此外利用梯度进行排序也非常具有启发意义。



## Abstract

神经网络在训练数据和测试数据的特征分布相似时能够获得不错的测试性能，但若特征分布差异较大（跨域挑战问题，cross domain）则泛化能力会大大降低。因此作者引入了一个自我挑战表达算法（Representation Self-Challenging, RSC），能够显着提高了CNN 对域外数据的泛化能力。 RSC 迭代地挑战（丢弃）在训练数据上激活的主要特征，并强制网络激活与标签相关的剩余特征。



## 1. Introduction

开头作者先举了个例子来说明人类学习特征的模式：

在课堂上教孩子区分猫和狗时，孩子可能一下子就能回答“猫有肉乎乎的脸”，随即停止学习，但是如果我们要求孩子说出更多特征，他们就会寻求更多的差异并不断学习，即使这一条线索就足够分类课本中的猫狗图片。而对于神经网络来说它的学习方式可能是找到了一条能够将数据分类的特征后就停止了学习，因为这一条特征在数据集（课本）中已经满足分类要求了。

如果现在的训练集和测试集的特征不同呢，那神经网络只学习一条特征很可能会导致糟糕的结果，为了训练一个对源域（训练集）和目标域（测试集）分布差异不同情况下性能不变的模型，域自适应机制出现了。而现在行业开始需要可以应用于训练阶段未见的领域的模型，域泛化能力作为域自适应的一种扩展也开始慢慢被研究。

在本文中，作者介绍了一种简单的启发式训练方法，可以提升跨域泛化能力。这种方法在每轮训练中丢弃与较高梯度相关的特征（representation），并强制模型使用剩余特征信息进行预测。这种启发式方法就像一种“自我挑战”机制，因为它抑制全连接层使用最具预测性的特征子集进行预测（即最明显的特征）。



## 2. related works

Domain Generation主要包括两大方向的工作：域不变特征的学习和源域数据增强

### 2.1 域不变特征学习

这通常会最小化源域之间的差异，假设结果特征将是域不变的，并且可以很好地泛化不可见的目标分布。RSC参考了该研究方向的一些成果，在不知道源域划分的情况下惩罚一部分特征以激活所有源域特征。

### 2.2 源域数据增强：

这些方法将源域扩展到更广泛的训练数据空间，扩大了覆盖目标域中数据跨度的可能性。相比之下，RSC 是一种与模型无关的训练算法，旨在提高任何给定模型的跨域鲁棒性。更重要的是，RSC 不利用域分区的任何知识，无论是源域还是目标域，这在现实世界场景中会应用的更好。

### 2.3 模型正则化

由于CNN常常在源域数据中过拟合，所以正则化方法得到广泛应用，作者在这里介绍了DropOut, CutOut, SpatialDropout, Adversarial Dropout等。RSC 与上述方法的不同之处在于，RSC 通过比较梯度而不是随机性来激活和抑制特征图的预测部分（权重）。这种选择过程在提高收敛性方面起着重要作用。



## 3. Method

数学符号：

(x, y) 表示来自数据集合 (X, Y) 的样本-标签对。

Z表示通过神经网络学习到的(x,y)的特征表示。

f(·;θ)表示CNN模型，其参数记为 θ。 

$$h(·;θ_{top}) $$表示$$ f(·;θ) $$的任务分量； $$h(·;θ_{top}) $$取Z作为输入并将输出作为softmax函数的输入。

 $$θ_{top}$$ 表示$$h(·;θ_{top}) $$的参数。

$$l(·,·)$$表示一个通用的损失函数。 

除此之外RSC 需要超参数p：表示百分比丢弃。

### 3.1 自我挑战算法

传统神经网络模型可以表示为：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831091221.png" alt="image-20210831091221720" style="zoom: 33%;" />

RSC的工作流程可以表示为在每次迭代训练时先检查梯度大小，找到相应的梯度，然后静音对预测影响最大的子特征z，最后更新整个模型。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831095704.png" alt="image-20210831095704087" style="zoom:50%;" />

以下为详细算法：

1. 定位

   首先计算最顶层关于特征表示的梯度，在这里⊙表示逐元素相乘：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831092313.png" alt="image-20210831092313173" style="zoom: 33%;" />

​	 然后将计算出的梯度$$g_z$$排序，得到100 − p百分位的数值记为$$q^p$$，然后按照下式得到掩码：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831095015.png" alt="image-20210831095015718" style="zoom:33%;" />

2. 静音

   对于每个特征表达z，RSC将较大的梯度根据掩码进行静音：

   <img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831095222.png" alt="image-20210831095222879" style="zoom:33%;" />

3. 更新

   将静音后的z用来计算softmax并反向传播更新梯度

   <img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831095403.png" alt="image-20210831095403150" style="zoom:33%;" />

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831095428.png" alt="image-20210831095428582" style="zoom:33%;" />



### 3.2 理论证据

回到一开始的例子，“猫有胖脸”这个特征足以对书本中的猫狗进行分类，而其他特征如高矮胖瘦也会对分类起到影响。当孩子要对世界上所有的猫狗进行分类的话则不能只看“猫有胖脸”这一个特征，而是需要把高矮胖瘦这些通过一些复杂的组合结合在一块分析。

而因为这些复杂组合在书本中也是包含了的，孩子所以不需要在现实中再学习这些特征，只学书本上的一样可以起到效果。

这种现象被称为领域自适应中的**协变量迁移（covariate shift）**，即条件分布（猫的语义信息）在每个域中都是相同的（书本和自然界的猫都有高矮胖瘦特征），但是由于边际分布不同（猫不同），模型学习到的特征也会有变化。

这部分的理论推导看的并不是很明白，故在此省去。



### 3.3 工程规范和拓展

该部分详细介绍了 ResNet 主干 + FC 分类网络上的 RSC 实现。 RSC 应用于训练阶段来对 ResNet 的最后一个卷积特征张量进行运算。将输入样本的特征张量表示为 Z，将其梯度张量表示为 G。 G 是通过反向传播相对于真实类别的分类分数来计算的。它们的大小都是 [7×7×512]。

#### Spatial-wise RSC

本文将此方法称为空间 RSC，这是本文的默认RSC。

1. 在训练阶段，在全局平均池化层沿**通道（channel）维度**应用梯度张量 G ，以产生大小为 [7 × 7] 的加权矩阵 w_i。

2. 使用这个矩阵，我们选择 7 × 7 = 49 个权重的百分位p作为阈值，将其在 Z 中的相应特征静音。 49 个权重中的每一个对应于 Z 中的一个大小为[1×1×512]的特征向量。

3. 之后，新特征张量 Z_new 被发送到softmax计算输出。
4. 最后，网络通过反向传播进行更新。

#### Channel-wise RSC

RSC 也可以通过删除具有高梯度的通道的特征来实现。通道级RSC背后的原因在于DNN的卷积性质。大小为 [7 × 7 × 512] 的特征张量可以被认为是输入图像的分解版本，其中每个像素有 512 种不同的特征。

1. 首先沿梯度G的**空间（spatial）维度**应用全局平均池化，并产生大小为 [1 × 512] 的加权矩阵。
2. 选择该向量的 512 个单元中的百分位p值，并将其在 Z 中的相应特征静音。这里，512 个单元中的每一个都对应于 Z 中的一个 [7 × 7] 特征矩阵。
3. 之后，新的特征张量 Znew被转发到新的网络输出。
4. 最后，网络通过反向传播进行更新。

显然可以看到这两种方法的整体思路相同，只是在选择特征向量时的维度不同。

### Batch Percentage

1. 随机法：该方法将 RSC **随机**应用于每批中的一些样本，而其他样本保持不变。 这引入了一个额外的超参数，即批次百分比：每批中应用 RSC 的样本百分比。 

2. 交叉熵损失法：使用一个最高百分比阈值p，将 RSC 应用于交叉熵损失值最高的p%的样本。 这种设置略好于随机性。



## 4. Experiment

### 4.1 数据集

1. PACS ：包括四个领域的七个类（艺术绘画、卡通、草图和照片）。实验在三个域上训练模型并在其余域上进行测试。
2. VLCS ：包括四个域的五个类。
3. Office-Home ：包括4 个领域（艺术、剪贴画、产品和现实世界）的 65 个类。
4. ImageNet-Sketch ：包括两个域的 1000 个类。 该实验在标准 ImageNet训练集上进行训练，并在 ImageNet-Sketch 上进行测试。

### 4.2 消融研究

#### 特征删除策略：

Top-Activation选择具有最高范数的特征，而Top-Gradient（RSC 中的默认值）选择具有高梯度的特征。对比表明，“Top-Gradient”优于“Top-Activation”，而两者都优于随机策略。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831132319.png" alt="image-20210831132319737" style="zoom:50%;" />

#### 特征丢弃百分比p 

本实验以不同的丢弃百分比p运行 RSC 以删除一部分空间特征。结果表明在 p = 33.3% 时达到了最高的平均准确度。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831132505.png" alt="image-20210831132504950" style="zoom:50%;" />

#### 批次百分比

RSC 可以选择仅随机应用于每个批次中的样本子集，本实验测试了不同批次百分比对结果的影响。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831132644.png" alt="image-20210831132644869" style="zoom:50%;" />

#### 空间RSC+通道RSC

这里作者将空间方向和通道方向 RSC 分别以 50% 的概率应用于样本，发现结果比单独使用空间RSC好。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831132832.png" alt="image-20210831132832537" style="zoom:50%;" />

#### 不同DropOut

Dropout 启发了许多用于 CNN 的正则化方法。这些方法之间的主要区别在于在输入数据、卷积层或全连接层上应用随机或非随机 dropout 机制。结果表明本文提出的基于梯度的 RSC 方法更好。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831133015.png" alt="image-20210831133015162" style="zoom:50%;" />



### 4.3 跨域评估

在表下表中作者将 RSC 与最新的领域泛化工作进行比较，例如 Hex、PAR、JiGen 和 MetaReg。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831133331.png" alt="image-20210831133331612" style="zoom:50%;" />

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831133342.png" alt="image-20210831133342281" style="zoom:50%;" />

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831133317.png" alt="image-20210831133317250" style="zoom:50%;" />

值得注意的是，对于 ResNet18 和 ResNet50，RSC 显着提升了草图域的性能，草图域是唯一的无色域，模型必须理解对象的语义才能在草图域上表现良好。



## 5. Discussion

这一部分主要说明了该方法在ImageNet数据集的benchmark实验，实验模型采用了相似架构的resnet50， resnet101，resnet152，结果如下：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210831133905.png" alt="image-20210831133905699" style="zoom:50%;" />

结果表明RSC 具有缩小相同系列但不同规模网络之间性能差距的能力（例如使用RSC 后ResNet50性能接近baseline ResNet101）。



## 6. Conclusion

本文介绍了一种简单的启发式训练方法，它可以直接应用于几乎任何 CNN 架构而无需额外的模型架构，也不需要多余的训练步骤。 RSC 迭代时强制 CNN 激活在训练域中不太占优势但仍与标签相关的特征。 RSC 的理论和实证分析证明，它不仅是扩展训练域特征分布的一种有效的方法，也可以直接用于简单的图片分类任务中（ImageNet）。
