# Improving Adversarial Robustness Using Proxy Distributions

论文地址：https://arxiv.org/abs/2104.09425v1

本文地址：https://github.com/Quinn777/Notes



## 写在前面

Princeton University，Vikash Sehwag. ICLR 2021 workshop

本文用已有的生成样本模型（如GAN）对原始数据分布进行拟合以生成新的输入数据，并从理论上研究了代理分布与原始分布的偏移、以及其带来的鲁棒性惩罚，证明了只要拟合的分布足够好, 那么鲁棒性的差距就能足够小。在实验中作者也证明了该方法是SOTA的



## Abstract

代理分布是训练数据原始分布的一个近似，它能够为我们提供无限可用于训练的样本来提高对抗鲁棒性。但是为什么在训练阶段使用代理分布中额外的训练样本会提高对抗鲁棒性呢？作者证明了分类器在代理分布和原始训练分布的鲁棒性上限受到Wasserstein距离的限制。另外实验结果也证实近似于训练数据的代理分布可以提高对抗鲁棒性。在实验中作者使用了SOTA的数据生成模型和不同的攻击方法，也尝试了不同规模的图像数据集，对对抗训练的准确性与稳健性权衡和样本复杂性进行了第一次大规模的实证研究。



## 1. Introduction

训练数据可以提高对抗训练的性能，但是在真实世界中图像往往难以获取，所以作者使用代理分布来解决这个问题。代理分布使用当前数据集中可用的训练图像进行建模，使用神经网络（如GAN）建模时的代理分布可用生存无限数量的高质量图像。

代理分布存在一个问题，即其只是训练数据分布的近似值，那通过训练代理分布样本实现的鲁棒性是否能够完全转移到原始数据集中呢？转移了多少？就此问题作者证明了在两个分布上鲁棒性的差异受限于他们之间的条件Wasserstein距离。

该发现说明对与训练数据非常接近的代理分布的样本进行训练时，可以有效提高模型原数据集的鲁棒性。在实验中也验证了即使只对与代理分布样本进行对抗训练，获得的模型对原始分布样本依旧具有鲁棒性。

在实验部分中，作者使用SOTA的生成模型来获得受到Wasserstein距离限制的代理分布样本，再同时对原始训练机和代理分布中采样以作为训练数据。实验结果表明本方法在对L族攻击模型中鲁棒准确度均达到SOTA，且与使用额外500000张数据的真实世界数据集的对照实验中，使用代理分布样本的组获得的鲁棒准确度表现的更好。

此外，为了探讨图像规模与鲁棒性之间的关系，作者只使用代理分布对2k-10m之间的数据集在多个网络上进行训练。然后还分析了准确性与鲁棒性的权衡，并且在使用更多样本时，我们可以降低这种权衡（即不用牺牲太多准确性）

### 贡献

- 提供了对不同数据分布之间对抗鲁棒性迁移的理论依据与实验验证，为代理分布与原始分布之间鲁棒性的差异提供了一个上限
- 将代理分布与对抗训练结合，实现了SOTA的鲁棒准确性
- 在2k-10m的图像上训练网络，对对抗训练准确率与鲁棒性权衡以及样本复杂性进行了大规模研究验证



## 2. Integrating proxy distributions in adversarial training

#### Notation

原始分布为$D满足D_{x \times y}$，代理分布为$\tilde D$，由生成模型获得。神经网络分类器表示为$f:X\to Z$，其中网络参数为$\theta$，输出概率为$Vectors(z)$，损失函数为交叉熵损失，表示为$l(.),l(\theta,x,y)=-log(f_\theta(x),y)$。$S$表示为从$D$中获得的样本，$\tilde S$表示从$\tilde D$中获得的样本。

对抗训练：

对抗训练的模板时最小化在对抗攻击中获得的对抗样本的训练损失，如基于投影梯度下降的攻击（PGD），模型如下：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928110216.png" alt="image-20210928110216236" style="zoom:50%;" />

其中$\Omega$为威胁模型的一些参数，如对抗扰动$\epsilon$，攻击轮次，每步攻击规模等



### 2.1 理解不同分布之间对抗鲁棒性的迁移



对于分类器$h:X \to Y$，首先定义了一个分类器的平均鲁棒性，其取决于条件Wasserstein距离。

#### 定义1:平均鲁棒性

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928111035.png" alt="image-20210928111035620" style="zoom:50%;" />

上式表示每个样本到最近的对抗样本的距离的期望（即求和）。以前研究中对于robust accuracy的定义为对给定距离内是否存在对抗样本，而本文中则是计算了最近的对抗样本的距离。

为了限制这个平均鲁棒性，作者对于任意一个学习算法$L$（如对抗训练），从代理分布$\tilde D$中采样$S$个样本（包含了n个标签）作为训练集，此时模型的平均鲁棒性变成了：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928111922.png" alt="image-20210928111922433" style="zoom:50%;" />

为了更好地理解上式，作者将其分解为：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928112123.png" alt="image-20210928112123761" style="zoom:50%;" />

加上期望后，<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928112318.png" alt="image-20210928112318001" style="zoom:50%;" />就变成了：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928112337.png" alt="image-20210928112337219" style="zoom:50%;" />

可以看出最终模型的鲁棒性取决于三个部分：经验鲁棒性、泛化惩罚和分布偏移惩罚

- 经验鲁棒性表示在训练数据为从代理分布中采样S时的鲁棒性，对模型来说即训练集的鲁棒性；

- 泛化惩罚表示不采样（使用所有分布数据）和采样（使用S）的鲁棒性之差，对模型来说即训练集与整个拟合的数据分布上的鲁棒性；
- 分布偏移惩罚表示使用原始分布数据和代理分布数据的鲁棒性之差，即人造数据与真实数据的鲁棒性之差

所以要使我们的平均鲁棒性更高，则需要同时限制泛化惩罚和分布偏移惩罚，但是因为之前的研究中证实泛化性是有一个独立边界的（即已知泛化惩罚限制在一个区间内），故我们只需要限制分布偏移惩罚就够了，而分布偏移惩罚与分类模型无关，仅仅与数据的分布有关。

#### 定义2:条件Wasserstein距离

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928121319.png" alt="image-20210928121318905" style="zoom:50%;" />

其中$J(D,\tilde D)$为$D, \tilde D$的联合分布。

两个分布之间的条件 Wasserstein 距离表示为每个类的条件分布之间的 Wasserstein 距离的期望。

**补充：**

> 沃瑟斯坦距离（Wasserstein Distance，简称为“W距离”）。W距离是最优传输（Optimal Transportation）理论中的关键概念，而最优传输及W距离的场景可以用一个非常的简单实际问题来描述：有一座土山，需要把它搬运到另外一个地方堆积成一座新的土山，要求整个过程中土的总质量不能改变。最优传输问题即在诸多的运土方案中，寻找代价最小的那个方案，而W距离指的就是这个最小代价。
>
> <img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928121720.png" alt="image-20210928121720734" style="zoom:50%;" />
>
> 对于上图有<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928121745.png" alt="image-20210928121745800" style="zoom:50%;" />
>
> ![[公式]](https://www.zhihu.com/equation?tex=\gamma(x_p%2Cx_q))为挪箱子数， ![[公式]](https://www.zhihu.com/equation?tex=\left\|x_p-x_q+\right\|) 即表示走过的位移。记所有传输计划的集合为![[公式]](https://www.zhihu.com/equation?tex=\Pi)，即![[公式]](https://www.zhihu.com/equation?tex=\gamma+\in+\Pi)，W距离定义所有传输代价中的最小者，即![[公式]](https://www.zhihu.com/equation?tex=W(P%2CQ)%3D\mathop{\min}_{\gamma+\in+\Pi}B(\gamma))
>
> 引用自https://zhuanlan.zhihu.com/p/358330515

#### 定理1：约束分布偏移惩罚

对于所有的标签$y^* \in Y$，从原始分布中选取一张图片将其分类为$y^*$的概率与从代理分布中分类为$y^*$的概率一定相同。并且对于任意的分类器h，有：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928164913.png" alt="image-20210928164913667" style="zoom:50%;" />

上式表明了分布偏移惩罚的上限。注意增多数据时分布偏移惩罚并不会减少，但是会降低泛化惩罚。下文的实验结果表明，当提供更多来自代理分布的样本时，泛化惩罚确实接近于零。

#### 推论2：

如果同时从代理分布和原始分布中抽取样本呢？这里令$\bar D=p\cdot D+(1-p)\tilde D$，其中p为原始分布的权重。则对任意分类器h有：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928170331.png" alt="image-20210928170331882" style="zoom:50%;" />

通常p非常小（代理分布中抽取的样本非常大），这表明包括来自原始分布的数据不应该对获得的分布偏移惩罚的界限有很大影响。

#### 定理3: 定理1的边界紧密性

任取$\epsilon<=Rob_d(h,D)$，任意距离d，任意分类器h，都存在一个代理分布使得：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928172229.png" alt="image-20210928172229410" style="zoom:50%;" />

该限制仅限于固定了分类器h的情况，此时仍然有可能存在优化算法L来获得更小的分布偏移惩罚



### 2.2 使用代理分布来提升鲁棒性

这里作者将代理分布应用到鲁棒训练当中，以提高原始分布数据的鲁棒性，最终在对抗训练中目标函数就变成了：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928174408.png" alt="image-20210928174408485" style="zoom:50%;" />

其中$L_{adv}$表明对抗训练算法，故替换后如下：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928174954.png" alt="image-20210928174954619" style="zoom:50%;" />

其中$\gamma \in [0,1],L_{smooth}(θ,x,y)=l(θ,x,y)+βD_{kl}(fθ(x),fθ(x+δ)), δ∼N(0,σ2I)$

$D_{kl}(.,.)$表示 KL-divergence.

作者在原始分布数据中使用交叉熵损失函数，而代理分布中使用KL-divergence loss，关于kl-divergence loss 可以看这篇文章https://zhuanlan.zhihu.com/p/95687720

本模型的原理图如下：

![image-20210928175410037](http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210928175410.png)

首先训练一个生成模型并从中采样大量合成图像。 接下来，从中过滤掉质量较差的合成图像。 最后，在过滤后的合成样本和原始训练样本的组合集上训练一个鲁棒的分类器。



## 3. Experimental results

### 3.1 通用实验设置

具体设置可以去文中看，这里总结一下实验流程：

1. 使用生成模型（StyleGAN/DDPM）来合成满足代理分布约束的数据
2. 使用BiT, SplitNet, LaNet来生成标签
3. 过滤低质量的生成数据
4. 使用L族威胁模型和PGD/AutoAttack来执行对抗训练



### 3.2 代理分布生成模型的选择

以下是用两种生成模型合成图像的对比实验，FID和Inception Score用于衡量图像质量

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210929181555.png" alt="image-20210929181555893" style="zoom:50%;" />

可以看到DDPM生成的样本质量没有GAN好，即图像不太逼真，但是在更有利于提高鲁棒性，故后续实验使用DDPM



### 3.3 使用代理分布

首先使用从无条件 DDPM 模型中采样合成图像，再使用 LaNet和SplitNet 模型对其进行标记（LaNet这里先挖个坑），然后按照1:1组合两个分布的图像进行对抗训练。即：

1. 使用DDPM生成代理分布数据
2. 利用LaNet与SplitNet生成两组预测标签
3. 去掉两组中预测结果不同的数据，再去掉两组中预测置信度小于90%的数据

#### 过滤低质量图像

在人工检查中，作者发现由其中有一些质量很差的图像，这种情况会稍微影响最后的训练结果。

如何过滤这些质量差的图像呢？作者用LaNet和SplitNet同时对图像进行预测，发现只有少量预测结果较差，故作者留下两个网络分类置信度都超过90%且预测结果相同的样本，剩下的则丢弃。注意这里没有用BiT的结果，因为它需要使用ImageNet做一次迁移学习，而我们不想让模型对已知数据产生依赖。

最后作者讨论了用LaNet、SplitNet和BiT打标签后，对于过滤与不过滤操作产生的准确率差异：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210929213811.png" alt="image-20210929213811094" style="zoom:50%;" />

上表中每一列分别表示原始模型、对抗训练后的模型、进行auto- attack的预测结果。

#### 对抗训练

![image-20210929220741114](http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210929220741.png)

用L族威胁模型做了一些对抗训练，并用auto attack进行攻击，实现了SOTA的性能，值得注意的是加入合成样本后原始精度也提高了（以前都是精度与鲁棒性二选一）

#### 经过认证的鲁棒性

作者将RST（随机平滑）和一些其他方法作为baseline与本方法进行比较，获得了更好的认证鲁棒精度

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210929222428.png" alt="image-20210929222428279" style="zoom:50%;" />

而且即使RST加入了额外500k张现实世界数据集后，效果依旧没有本文的好



### 3.4 应用在大规模数据集上

![image-20210929222829176](http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210929222829.png)

从上图中可以看到，在样本数量非常少的情况下，对抗训练中的clean准确度被用来实现稳健性。然而，随着训练样本数量的增加，这种clean accuracy与robustness的权衡越来越弱，且即使对于小网络这种规律也存在。



## 4. Conclusion

我们使用合成数据使深度神经网络对对抗性攻击更加鲁棒。然而，合成数据是从代理分布中采样的，即仅近似于训练数据的基础数据分布。因此，第一个关键问题是合成数据是否有助于提高鲁棒性。

本文就研究了从代理到原始训练数据分布的鲁棒性性转移，并提供了一个严格的上限。实验结果验证了与训练数据分布非常接近的代理分布也能够提高鲁棒性。

为代理分布选择生成模型时，作者认为在其进程中存在一个拐点，在此到达这个拐点之后生成模型就可以充分捕获数据模式，从而生成逼真和多样化的样本集。在 CIFAR-10 数据集上，作者发现 StyleGAN 和 DDPM 模型都超过了这个拐点，故两者的样本都提高了性能。然而，在 ImageNet数据集上SOTA的BigGAN-deep 模型没有任何性能提升，这表明该模型离 ImageNet 数据集上的拐点还很远。

