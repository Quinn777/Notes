[toc]



# HYDRA: Pruning Adversarially Robust Neural Networks

论文地址：https://www.cs.columbia.edu/~tcwangshiqi/docs/hydra.pdf

源码地址：https://github.com/inspire-group/hydra



## 写在前面

目录拉到最底了，conclusion也看完了，脑子里却啥都不剩了 = 。= 

为了避免以上惨案再次发生，我痛定思痛决定开始写笔记，正好最近导师扔了一堆文章过来，于是趁着假期的一点闲暇尝试开辟自己的第一个专栏，未来会在这里分享一些权重剪枝与对抗鲁棒性相关的论文解读，欢迎大家一起交流心得～

Hail Hydra!

这是Princeton INSPIRE 实验室发表在NIPS 2020上的一篇文章，作为在知乎的第一篇阅读笔记，当然得拥有一个炫酷的名字（不得不佩服大佬的脑洞），这项研究将CNN剪枝方法与对抗训练结合考虑，开发出了一种全新的剪枝策略，并探究了鲁棒网络中的一些性质。



## Abstract

### 深度学习面临的两个关键挑战

* 缺乏对抗性攻击的鲁棒性
* 参数量过大。

### 对应解决方案

* 鲁棒性训练
* 网络剪枝

### 抛出问题

缺乏对两个问题对联合研究，且剪枝策略与鲁棒性训练技术（对抗训练和可验证性鲁棒性训练）集成时表现不佳

### 我们的解决方案

*让剪枝技术明确鲁棒性的训练目标，并让训练目标指导搜索要剪枝的连接。*作者将剪枝目标表述为一个**经验风险最小化问题**来实现这一观点，该最小化问题通过SGD得到有效解决。

实验中作者通过四种鲁棒训练技术：迭代对抗训练、随机平滑、混合训练和CROWN-IBP和三种数据集：CIFAR-10、SVHN和ImageNet来展示方法的性能。

作者还证明了在非鲁棒网络中存在高度鲁棒的子网络。



## Introduction

### 网络剪枝

神经网络剪枝的原理是最低权值(the lowest weight magnitude (LWM)对网络性能影响最小

另外一种剪枝方法：alternating direction method of multipliers (ADMM) based optimization.

### 缺点

LWM和ADMM在良性训练中表现较好，但在对抗训练中表现较差

### 思路

作者认为更好的方法不是继承剪枝启发式算法并将其应用于所有鲁棒训练目标，而是在剪枝时将鲁棒准确率**作为训练目标**来寻找要修剪的权重。

> Instead of inheriting a pruning heuristic and applying it to all robust training objectives, we argue that a better approach is to *make the pruning technique aware of the robust training objective itself*.

文章通过制定剪枝步骤来实现这一点。作者将“决定剪枝哪些连接”作为一个具有鲁棒训练目标的经验风险最小化问题（an empirical risk minimization problem with a robust training objective），然后使用随机梯度下降（SGD）来解决。

在修剪步骤中优化每个连接的重要性分数，同时保持微调步骤完好无损。重要性得分最低的连接将被删除。

我们提出了一种重要度评分的scaled initialization，这是我们压缩网络的高良性和鲁棒准确性背后的一个关键驱动因素。

### 主要贡献

- 通过将其公式化为经验风险最小化问题，开发出一种新的剪枝技术，该剪枝策略能够注意到鲁棒性训练的目标。作者采用了一种基于**重要性评分**的优化技术，并提出了重要性评分的**比例初始化**（scaled initialization of importance scores）。
- 我们评估了四个鲁棒性培训目标，即迭代高级训练、随机平滑、混合训练，并在CIFAR-10、SVHN和ImageNet数据集上应用多个网络进行训练。值得注意的是，在99%的连接剪枝率下，与之前分别针对ImageNet、CIFAR-10和SVHN数据集的工作相比，我们在**鲁棒准确度方面实现了高达3.2、11.2和17.8个百分点的增益**，同时实现了最先进的良性准确度。
- 我们还**证明了非鲁棒或弱鲁棒网络中存在高度鲁棒的子网络**。特别是，在没有可验证鲁棒性的经验鲁棒网络中，我们能够找到具有已验证鲁棒精度的接近最新技术的子网络。这句话好绕。



## Background and related work

### 鲁棒性训练

包括两种，对抗性训练和可验证的鲁棒性训练

#### 对抗训练-Adversarial training

目标：最大限度地减少通过迭代对抗性攻击（如PGD攻击）获得的对抗性示例的训练损失。

#### 可验证的鲁棒训练-Verifiable robust training

本文使用了以下三种可验证性鲁棒训练方法（先挖个坑，以后一定看233

- 基于线性松弛的混合训练法 MixTrain based on linear relaxations
- CROWN-IBP based on interval bound propagation (IBP)
- 随机平滑randomized smoothing



### 神经网络剪枝

包括LWM和ADMM



### 鲁棒训练中的剪枝

#### 别人的做法

- 使用基于LWM的剪枝启发式算法来实现经验对抗鲁棒性；
- 采用了交替方向乘数法（ADMM）剪枝框架结合基于LWM的剪枝启发式算法，以实现压缩网络更好的经验鲁棒性。

#### 本文的做法

让鲁棒的训练目标本身决定修剪哪些连接。

#### 特点

- 第一个研究具有可验证鲁棒训练的网络修剪，其中实现了具有高可验证鲁棒精度的高度剪枝网络
- 第一个在imagenet数据集上研究压缩模型鲁棒性



## Approach

相比较传统剪枝策略，更好的方法是**对具有期望剪枝率的神经网络进行架构搜索**，与预先训练的网络相比，该方法的目标精度指标下降最少。



### 剪枝——与鲁棒训练目标结合的经验风险最小化问题（ERM）

鲁棒性剪枝可以看作一个具有对抗性损失目标的经验风险最小化问题（ERM）。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210808205924.png" alt="image-20210808205924501" style="zoom:50%;" />

m为掩码mask，θ为参数，θ⊙m为掩码和权重参数之积。网络剪枝比例表示为$(1-K/N)$​​​，其中k为预先设置的阈值，$N=|θ_{pretrain}|$​​为预训练模型中的参数总数。

- 该算法中通过修改$L_{pruning}=L_{adv}$或者$L_{pruning}=L_{ver}$​来选择鲁棒性训练目标

- 而D则通过SGD优化最小经验损失来得出

- 最终得出的m会被用于微调阶段

**注意：**$L_{adv}$指的是对抗训练的损失函数，$L_{ver}$​​指的是可验证鲁棒训练的损失函数



### 基于重要性评分的优化

作者认为难以对mask值进行优化，因为m是二进制数。故对每个权重赋予一个浮点类型的重要性分数，将对mask的优化问题转化为对每个权值重要性分数的优化问题。

在进行预测时，只选择重要性得分最高的前k个权重。但是，在反向传播时，它将用梯度更新所有分数。



### 比例初始化（Scaled-initialization）

从上面可以知道剪枝参数是否合理取决于重要性分数的优化结果，而在SGD优化时一般都采用随机初始化方法，收敛速度较慢。

为了克服这一挑战，作者提出了重要性分数的比例初始化方法，该方法与预先训练的网络权重成比例地初始化重要性分数。通过比例初始化，我们在开始时更加重视大权重，并让优化器找到一组更好的修剪连接。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210809172928.png" alt="image-20210809172928823" style="zoom:50%;" />

$θ_{pretrain,i}$​​​表示第i层的权重，作者将权重映射到[-1, 1]的区间内。

$S_i$与右边的式子成正比，比例系数采用了<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210809174409.png" alt="image-20210809174409621" style="zoom:50%;" />



### 非鲁棒网络中的鲁棒子网络

作者在训练过程中采用了不同的损失函数。例如在预训练阶段损失函数设为$L_{benign}$，而剪枝阶段设为$L_{ver}$，则可以在一个良性非鲁棒网络中搜索到可验证鲁棒性的子网络



## Experiments

### 参数

1. dataset：CIFAR-10, SVHN, ImageNet

2. model：VGG-16, Wide-ResNet-28-4, CNN-small, CNN-large

3. 扰动阈值（perturbation budget）：

- 对抗训练：CIFAR-10和SVHN下设为8/255，ImageNet设为4/255

- 可验证鲁棒性训练：都设为2/255

4. 攻击函数：PGD attack，50步攻击，10步重启（We used PGD attacks with 50 steps and 10 restarts to measure *era*）



### 指标

1. Benign accuracy：良性准确率，即对未修改的图片的分类准确率
2. Empirical robust accuracy (era)：指使用PGD攻击生成的对抗性示例时分类正确的百分比
3. Verified robust accuracy (vra)：经验证的鲁棒性准确率。vra-m、vra-t和vra-s分别对应于从MixTrain、CROWN-IBP和随机平滑法中获得的vra



### 算法流程

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210810111112.png" alt="image-20210810111112167" style="zoom: 50%;" />

Step1：使用预训练损失优化目标$L_{pretrain}$进行训练，得到使loss值最小的模型权重

Step2：对使用scaled-initialization对每一层权重评分进行初始化，得到s

Step3：使用$L_{prune}$​​​​作为优化函数，将s作为参数进行优化，得到使loss值最小的权重评分$\hat{s}$​​

Step4：根据$\hat{s}$​构建掩码，p为剪枝率，k=100-p，$\hat{s_{k}}$​表示从小到大k百分位的分数，当$\hat{s}>|\hat{s_{k}}|$​​​时将分数对应权重的掩码设为1（即该权重会被剪去）

Step5：使用$L_{finetune}$​​​作为优化函数，将加上掩码的权重进行优化，得到最终剪枝后的权值。



### 一系列实验

#### 用更强的攻击验证经验鲁棒性

为了进一步确定压缩网络中的鲁棒性不是由梯度掩蔽等现象引起的，我们使用更强的PGD攻击（多达100次重启和1000次攻击步骤）以及基于梯度和无梯度攻击的组合对其进行评估。

#### 与从头开始训练相比

如果目标是实现一个压缩且健壮的网络，那么一个自然的问题是为什么不从头开始在一个紧凑的网络上进行训练，作者对此进行尝试并发现效果并不好。

#### 与Adv-LWM法相比

即剪枝采用LWM法，鲁棒性优化采用Adv（对抗训练，如PGD）。

#### 与Adv-ADMM法相比

即剪枝采用ADMM

#### 消融研究

作者在剪枝步骤中改变用于求解ERM的数据量，发现虽然对于少量图像求解结果变化不大，但在训练数据增大到10%左右时（CIFAR-10上的5k图像）中会发生明显的性能提升。此后，随着数据量增多将显著改善分类结果（附录B.2）。

接下来，作者将剪枝步骤中的纪元数从1个更改为100个，观察到即使是少量的修剪轮次（如五轮）也足以实现较大的对抗分类精度收益。并且随着剪枝轮次的增多会产生边际效益，收益将会越来越少。



### 实验总结

1. HYDRA比以前的作品（包括Adv LWM和Adv ADMM）同时提高了良性精度和鲁棒性
2. HYDRA在多种扰动强度下都提高了性能（见图1）
3. HYDRA比以前的技术提高了压缩比
4. HYDRA具有强大的泛化能力，实验中通过四种不同的鲁棒训练技术（第4.2节）都实现了最先进的性能。



## Hidden robust sub-networks within non-robust networks

该部分中作者将预训练阶段和剪枝阶段的训练目标设为不同以期望看到不同的结果。

作者在实验中使用三个不同的训练目标（即良性训练、对抗性训练和随机平滑）来生成**预训练网络**。其中使用VGG16网络和CIFAR-10数据集，每个网络的修剪率为50%。

实验结果如下：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210810161009.png" alt="image-20210810161009740" style="zoom:50%;" />

*实验结果表明，即使在非鲁棒网络中也存在高度鲁棒的子网络。*

作者说他们的实验中使用良性训练的预训练网络中era=0%（第一行），而在后续对剪枝网络采用对抗训练时可以得到43.5%的era，相比之下第二行采用对抗训练得出的预训练网络era=51.9。

另外一个令人惊讶的实验结果是在经过对抗训练的预训练模型中，使用随机平滑训练子网络获得了更好的效果（63.6），相比之下直接使用随机平滑预训练的vra=61.6

，其再经过随机平滑训练的vra-s也只有60.7。



## Delving deeper into network pruning 

### 剪枝参数可视化

作者将预训练网络的权重、经过LWM剪枝后的权重和HYDRA剪枝后的权重进行可视化，并对比了结果。

![image-20210810163730106](http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210810163730.png)

发现：

1. 在进行剪枝参数优化时可能会发现不必要的小规模连接，并将其删减；
2.  然而，与LWM启发式算法相比，本文的方法也倾向于修剪一些较大的权重，而不是较小的权重。



### 集成量化技术后可实现进一步压缩

我们发现我们的剪枝网络（即使在99%的剪枝率下）可以量化8位，而良性和鲁棒准确率仅降低<0.5个百分点。这在已经被大量删减的（10x-100x）网络中又增加了了4倍的压缩系数。



### 多轮修剪

为了减少计算开销，到目前为止，我们只使用了一个修剪步骤。在ImageNet数据集上，即使我们只使用单步修剪，结果也优于使用多步骤（20步）Adv LWM技术。



### 结构化修剪

结构化修剪，即修剪过滤器而不是连接，这对性能有更大的负面影响。当使用LWM技术修剪50%的过滤器时，VGG16网络的era在CIFAR-10上从51%降低到34.7%。我们的方法实现了38.0%的era，同时也比Adv LWM高1.1个百分点。



### 对于过度参数化的网络era的退化程度较低

对过度参数化的WRN-28-10网络进行90%的修剪，我们观察到era中只有0.3个百分点的退化，这明显低于较小的WRN-28-4网络产生的1.4个百分点的退化。



## Conclusion

在这项工作中，作者研究神经网络修剪和鲁棒训练目标之间的相互作用。作者主张通过将剪枝表述为一个优化问题，将鲁棒训练目标整合到剪枝技术本身中，同时在不同的数据集、网络体系结构和鲁棒训练技术中实现了最佳的良性和鲁棒准确度。一个开放的研究问题是进一步缩小未修剪和修剪网络之间的性能差距。