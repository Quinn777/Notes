# Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries

论文地址：https://arxiv.org/abs/2104.08382

代码地址：https://github.com/arjunbhagoji/log-loss-lower-bounds

## 写在前面

这篇文章发表于ICML 2021，作者为普林斯顿大学的Arjun Nitin Bhagoji等人（和HYDRA的作者一个团队）。

本文提出了一种在对抗样本中寻找交叉熵最小界限的优化框架，不仅可以用来作为评价鲁棒训练效果的工具，还将凸优化问题求解速度提高了3000倍。不过可惜的是目前这种方法仅仅用于二分类问题。



## Abstract

有监督鲁棒训练的局限性最近引起了人们的广泛兴趣，尤其是确定训练损失的分类器不可知边界（classifier agnostic bounds on the training loss）以知道什么时候可以学习。在本文中确定了在测试集中存在对抗样本时交叉熵损失的最优下限，以及相应的最优分类输出。作者将边界的公式转化为一个优化问题，适用于所有包含软分类器输出的损失函数，并给出了详细的证明。最后，作者研究了使用最佳分类输出作为软标签来改进鲁棒训练的可能性。

意义：作者说确定损失函数的下限可以帮助我们确定当前鲁棒训练方法的有效性、找出当前方法下模型与最优性能的差距。



## 1. Introduction

在有监督学习中分类器对对抗性扰动的鲁棒性已经成为一项重要的研究方向，在此之前已经有很多工作来在机器学习中（使用0-1 loss）对带有对抗样本的测试集中测试学习界限。然而，神经网络常常使用替代损失函数（surrogate loss functions）如交叉熵损失来进行预测，在这种函数上建立界限就十分困难了。

因此本文将测试学习界限的工作推广到任何使用分类器输出概率的损失函数。即回答了一个关键问题——在给定数据分布和对抗扰动规则时，任何分类器产生的最小可能交叉熵损失是多少？

具体做法是关注数据点之间的相互作用。作者将每个类的点表示为图的顶点，如果两个顶点之间可以扰动的领域重叠了，则称这两个顶点之间存在边，称这种结构为“冲突图”（conflict graph）。

作者将这个问题转化为在此冲突图中找到最佳分类器的输出概率。其下限由该图上的交叉熵损失确定，被称为图熵。

上述问题是一个凸优化问题，但针对此点传统解法计算十分缓慢，特别是对现实世界中复杂的特征分布。因此作者推导出一种自定义算法，利用冲突图的二分结构，可以比通用凸求解器更快地确定边界（10s以内）。

除此之外，作者使用这种方法发现使用0-1 loss的收敛能力远低于交叉熵loss，但是损失函数对收敛gap的影响并不大，主要影响因素还是模型架构。（好奇他是怎么得出这个结论的）。

最后作者由根据此提出了一个对抗训练的方法，即用寻找损失函数下限过程中得到的soft label来进行训练，并给soft label中的正确概率设置一个下限，这可以帮助网络找到一个更好的梯度。

 总结一下，本文的贡献主要有以下几点：

1. 对于在测试集包含对抗样本时使用输出概率的凸损失函数（交叉熵），开发出一个通用的求解下界的框架；
2. 一个更有效的求解对数损失的优化方法；
3. 分析了最近的几种鲁棒训练方法的有效性，并利用上面框架里计算出的最佳分类概率作为软标签，达到提升模型鲁棒性的效果。



## 2. Lower Bounds on Cross-Entropy Loss

本节描述了一种框架来计算在对抗攻击下交叉熵损失的下界。该框架可用于普通离散分布和两种高斯混合分布下下对于带有对抗扰动样本的二分类问题。

### 2.1 问题描述

x表示X分部空间下的输入样本图片，y=1或-1，表示二分类下的标签，而f描述x到y的映射关系，即分类函数。这里作者引入了一个“软分类器”h，h的范围在0-1之间，其值表示分类器将x分为每一类的概率（即“soft label”，如$y_1=0.9$时，$y_{-1}=0.1$）

#### 测试时对抗

在测试时加入对抗样本进行攻击，用以测试模型的对抗鲁棒性。公式$\tilde{x}=N(x)$描述了一个对抗样本，其中$N(.)$表示一个非空的邻域函数，该邻域的大小是由$l_p$系列来限制的。

##### 补充1：对抗样本的生成

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921143733.png" alt="image-20210921143733742" style="zoom:50%;" />

对于上图中的案例，$x^0$很显然会被分类为猫，因为在训练时我们会最小化预测标签与正确标签间的损失，导致它的分类结果和正确标签足够接近（此时优化目标为$\theta$）。但是在生成对抗样本时，我们的优化目标变成了找到一个$x'$使其预测标签与正确标签距离最远（此时优化目标为x），同时要求其与错误标签尽可能接近。

这是因为我们攻击的网络是别人已经训练好的，即 $\theta$是不会变的了，我们唯一能做的就是通过最小化损失函数，来找到一张能够让预测标签和真实标签足够无关的 ${x}’$。

##### 补充2：邻域函数

但是，作为对抗攻击，我们有一个额外的限制，那就是我们通过训练找出来的输入图片 ${x}’$ 和真实图片足够接近。换句话说，我们希望找出来的图片和真实图片肉眼看起来很相似，但是一旦放入神经网络识别器里，得出的却是错误答案。因此，真实图片 $x^{0}$ 和输入图片 ${x}’$ 的距离应该小于某一个阈值d。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921144856.png" alt="image-20210921144856245" style="zoom: 25%;" />

如何找到这个距离d呢，这就引出了邻域函数，见上图。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921145058.png" alt="image-20210921145058554" style="zoom: 33%;" />

我们有两种常见的方式：L2-norm 和 L-infinity，即文中提到的$l_P$族邻域函数，二者示意图如上。

例如在L2-norm 中就是：我们有一个输入 $x^0$，我们希望所有更新得到的值，均在圈圈以内（满足限制条件）。如果此时当我们通过梯度下降算法，得到一个 $x^t$ （蓝色的点）是在圈圈之外（不符合限制条件），那么我们就用一个符合限制条件的并和 $x^t$ 最接近的点来替代 $x^t$（蓝色的点），最终得到新的 $x^t$ （橙色的点）。

参考链接：https://alberthg.github.io/2019/03/27/Adversarial-attack/

#### 损失函数

在之前对于损失函数在对抗攻击中下界的讨论使用的都是0-1loss，但是目前交叉熵loss更为常见，作者在此就第一次使用交叉熵损失进行下界研究。$$l^{CE}(h, v) = − log h(x)_y$$表示二分类问题中的交叉熵损失函数，其中$$v=(x,y)$$表示一个“数据-标签”映射关系，$h(x)_y$表示软分类器将x分类为y的概率，可以理解成预测概率，只不过把硬标签换成了软标签。

至此，鲁棒分类问题的损失函数优化问题就变成了：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921150431.png" alt="image-20210921150431797" style="zoom:50%;" />

即，在给定离散分布P下使用领域函数$H(.)$生成对抗样本$\tilde x$，并在所有样本中最小化交叉熵损失的上界。

### 2.2 将下界作为一个凸优化问题

作者首先定义一个冲突图，以便将寻找下界的问题转化为该图顶点上的优化问题。 然后证明了可行的输出概率集是由冲突图的边关联矩阵确定的。 最后，作者通过最小化这个可行集来确定交叉熵损失的下限。

#### 冲突图

$$ G = (V , E ) $$描述了不同类之间点的关系，从中可以得出不同点邻域的交集。V表示顶点集而E表示边集。$v=(x,y)$即一个顶点，注意这里顶点表示的是一对$x\to y$关系。这里将边表示为$\varepsilon=((x,1),(x',-1))$，当且仅当$N((x,1)\cap(x',-1))$非空时边存在。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921232230.png" alt="image-20210921232230754" style="zoom:50%;" />

如上图所示，x1，x2对应的标签为1，x3，x4对应的标签为-1，上下两个半区组成了一个二分图。图中的实线表示两点之间存在边，而蓝色与红色圆圈表示不同的邻域空间。可以看到只有蓝色区域与红色区域有重合时边才存在，此时模型对两点的判断就会被干扰。

#### 定义1

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921232459.png" alt="image-20210921232459711" style="zoom:50%;" />

$q_v$表示模型对于$v=(x,y)$的软分类概率的最大值，即模型输出的置信度最大值。

#### 引理1

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921233141.png" alt="image-20210921233141647" style="zoom:50%;" />

其中q表示分类器将数据分类为正确标签的概率，<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921233245.png" alt="image-20210921233245618" style="zoom:50%;" />，其中$I$为单位矩阵，$Iq\le1$表示概率恒小于1。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210921233517.png" alt="image-20210921233517011" style="zoom:50%;" />表示图的关联矩阵，行表示边，列表示顶点。我们假设e1表示连接x1-x3的边，e2表示连接x1-x4的边，e3表示连接x2-x4的边，则以上面的示意图为例可以得到这样的矩阵：
$$
E = \begin{pmatrix} 1 & 0&1& 0\\ 1 & 0&0&1\\0&1&0&1\end{pmatrix}
$$


设x1-x4的概率分别为q1-q4，则上述引理可以表示为
$$
Eq=\begin{pmatrix} 1 & 0&1& 0\\ 1 & 0&0&1\\0&1&0&1\end{pmatrix}\begin{pmatrix} q_1 \\q_2\\q_3\\q_4\end{pmatrix}=\begin{pmatrix} q_1+q_3 \\q_1+q_4\\q_2+q_4\end{pmatrix}\le1
$$
证明：

以$q_1+q_3\le1$为例，$x_1,x_3$都是$e_1$边上的顶点，在两邻域交集上一定存在一个x，使$q_1\le h(\tilde x)_{-1},q_3\le h(\tilde x)_{1}$(因为x1到-1的距离一定比交集上的点到-1远)，且$h(\tilde x)_1+h(\tilde x)_{-1}=1$ ，所以可得 $q_1+q_3 \le 1$

#### 定理1-交叉熵损失的下界

设P为二分类样本的概率分布，N()为邻域，G为冲突图，包含了顶点v和边e，$p\in R^v $并且有$p_v=P({v})$表示样本实际的正确分布概率，而q为预测概率。则有下列优化问题：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922100606.png" alt="image-20210922100606793" style="zoom:50%;" />

在此限制条件下最小化q的值，最终可以得到一个软分类器$h^*$,可以获得预测正确的概率$q^*$，且对于所有h都有：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922101622.png" alt="image-20210922101622282" style="zoom:50%;" />

不等号右边是不加约束条件下原本对于鲁棒训练的交叉熵损失，左式在加约束后会比原来的损失更少，作者把这个值作为“下界”，即鲁棒训练中能达到的交叉熵最小值。

对于0-1损失，优化目标就变成了<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922103202.png" alt="image-20210922103202009" style="zoom:50%;" />

#### 引理2

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922111722.png" alt="image-20210922111722896" style="zoom:50%;" />

以上是经过优化后得到的最优q的性质。其中向量z为对抗攻击时，最优对抗样本预测正确的概率。



### 2.3 高斯分布时的情况

设$z^*$为对抗扰动，在高斯分布下交叉熵变成了<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922112923.png" alt="image-20210922112923245" style="zoom:50%;" />，其中$w^*=2\sum^{-1}(\mu-z^*)$

#### 定理2

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922113421.png" alt="image-20210922113421222" style="zoom:50%;" />

因此，对混合了两种高斯分布数据分类时，交叉熵的下界可以表达为上式



## 3. Efficiently Computing Lower Bounds

第二部分介绍了交叉熵下界的优化模型，为了解决这个凸优化问题作者定制了一个算法，可以做到比现有凸求解器更快的计算速度。

### 3.1 算法概述

该算法（OptProb）首先猜测将单个正确分类到第 1 类的所有顶点的概率，以及第 -1 类顶点的正确概率。这些概率反映类别分布的相对频率。 该算法求解线性规划问题，并根据最佳正确分类概率是大于还是小于猜测，来确认初始猜测分区的正确性。 在后一种情况下（小于猜测），算法递归地应用于两个子问题，并将它们的解决方案组合成原始问题的解决方案。

OptProb 在递归的每个阶段使用线性规划函数 LinOpt。 函数 LinOpt(A, B, E , P ) 求解一对对偶线性规划问题：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922155112.png" alt="image-20210922155112149" style="zoom:50%;" />



如果$P(A)>0,P(B)>0$时有：<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922160726.png" alt="image-20210922155959080" style="zoom:50%;" />

否则$r_v=P(\{v\})$

论文里并没有给出A，B的定义（我猜测是想表示二分图两个部分的顶点集

具体算法流程如下：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922160349.png" alt="image-20210922160349152" style="zoom:50%;" />

集合$A^-,A^+,B^+,B^-$表示二分图不同部分中预测为y=1或y=-1的顶点集。定义如下：
$$
A^+=\{v \in A:y_v=1\}\\A^-=A-A^+ \\ B^+=\{v \in B:y_v=1\}\\B^-=B - B^+
$$

### 3.2 算法最优性证明

#### 定理3

> 该算法可以得到定理1最小化的结果，即表示分类器正确分类的概率的最优值。

定理 3 的证明反映了 OptProb 的递归结构，并对顶点数使用归纳法。 它依赖于两个技术引理（依据附录 B 部分的证明）。 引理 4 建立了在每次迭代中求解的线性程序的解的性质。 该证明使用线性规划的标准对偶性和互补松弛论据。

#### 引理4

函数LinOpt(A,B,E,P)所输出的$(A^+,A^-,B^+,B^-,z)$有如下性质：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922163726.png" alt="image-20210922163726211" style="zoom:50%;" />

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922163843.png" alt="image-20210922163843455" style="zoom:50%;" />

#### 引理5

对$[k]=\{0,1,...,k-1 \}$，有$a:A \to[k],b: B\to[k]$，然后算法返回的$(q,z)$有如下性质：

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922165157.png" alt="image-20210922165157936" style="zoom:50%;" />

引理 5 确定了 OptProb一定会终止，并详细描述了最优 (q, z) 的结构。 两个顶点集都被分割成具有成对部分的分区，每对部分都类似于一个完整的二分图。 每个部分之间的边受顺序关系的限制。



### 3.3 复杂度

最坏情况下，在算法的每一步只有一个顶点会从二分图中的一个部分中被删除，并且只有当图中只剩下单个部分时，算法才会终止。这种情况下如果图中还有顶点则会进行递归，每次递归的复杂度为:
$$
O(|V||E| log(|V|^2/|E|)) 
$$


## 4. Experiments: Using Bounds as a Diagnostic Tool

### 4.1 在现实世界数据集中的鲁棒性下限

数据集：MNIST，Fashion MNISRT，CIFAR-10

每个数据集有十个类别，作者将其按照3:7分为两个类别来模拟二分类问题，每一类使用5000个样本。

对抗样本生成时使用的邻域函数为$l_2-norm$，扰动预算为$\epsilon$。

最顶层的LinOpt函数使用的是Scipy在2020年提出的最大流算法，而在递归时使用了Edmonds-Karp最大流算法。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922171741.png" alt="image-20210922171740936" style="zoom:50%;" />

上图为在l2对抗下使用不同扰动预算和样本数量时交叉熵损失下限的结果。

#### 下限数值

上图可知，在较小扰动情况下最优优化器几乎可以达到0损失，作者认为这并不能推广到测试数据，并得出结论：在MNIST扰动3.0和CIFAR-10扰动4.0以后下限值才是有用的

#### 二次采样的影响

上图可知，随着样本数量的增加，下界也会增加，这表明样本之间存在更多的交叉点，从而使对手具有更大的灵活性。

#### 运行时间

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922173042.png" alt="image-20210922173042120" style="zoom:50%;" />

作者将其与CVXOPT（Andersen et al.，2013）——一种具有非线性目标函数的线性规划通用求解器进行时间比较，后者使用的是原始-对偶内点法（Boyd et al.，2004）。

如上图所示本文的算法能够起到3000倍以上的加速效果。



### 4.2 人造高斯分布数据

这里使用对角协方差矩阵$\sum$，其中$\sum _{ii}$在0-1之间随机采样。设置高斯分布参数$\mu_i=C*\frac{\sum _{ii}}{\sqrt d}$，其中C是确定协方差之间距离的常数。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922184115.png" alt="image-20210922184108643" style="zoom:50%;" />

图中比较了全部样本损失的下限和在d=100时进行采样（k个样本）产生经验分布的下限，可以看到后者下限明显更低。

在后一情况下经验分布在较低预算（小于3）时缺乏交集的原因是，在高维中，即使基础分布重叠，采样点邻域之间的交集也需要更多的扰动点。



### 4.3 评估鲁棒训练的性能

#### keywords

- 标准对抗训练可以通过增大网络实现接近最小的交叉熵损失，但 0-1 损失仍然存在差距
- 训练时产生的具有最佳概率的软标签可以帮助缩小这一差距，并在某些情况下有助于泛化。

#### 鲁棒训练能达到的最低损失是多少

本节里通过使用对抗训练和TRADES等鲁棒训练技术来获得交叉熵损失，并将其与我们的下界相比较，展示了不同鲁棒训练方法与最优损失值的差距。

对抗训练使用AutoAttack来计算鲁棒交叉熵损失，使用one-hot标签，即硬标签，而TRADES使用网络预测的置信度作为软标签。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922185349.png" alt="image-20210922185349503" style="zoom:50%;" />

在图中，对于两个数据集，对抗训练在训练数据上实现了接近最小边界的交叉熵损失。

一个有趣的发现：带有硬标签的标准对抗训练优于 TRADES。这与之前研究中在较低对抗性预算下观察到的 TRADES 更鲁棒相反。

除此之外，作者对不同模型和不同激活函数也做了消融实验，具体可以看文章附录部分。

#### 使用软标签

众所周知软标签可以提供神经网络性能，但是在较高扰动时，分类器会给错误类别打出更高的置信度，因此获得的软标签也是有噪声的。

为了避免引入这种标签噪声、提取有意义的梯度，作者对正确类别的概率加入了一个下限作为限制条件。

<img src="http://xiangkun-img.oss-cn-shenzhen.aliyuncs.com/20210922190352.png" alt="image-20210922190352346" style="zoom:50%;" />

上述实验结果表明这种方法可以有效降低损失、提高鲁棒性。



## 5. Related Work

暂不展开



## 6. Discussion

在本文中，作者提供了一个框架来计算一般离散分布和高斯混合的交叉熵损失的最佳下界，并且展示了如何利用这个框架来分析当前训练方法的有效性。 

在未来工作中，作者计划将该框架扩展到所有连续概率分布以及多分类情况。 
